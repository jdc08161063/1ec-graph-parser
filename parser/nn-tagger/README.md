# Spine tagger

This folder contains a tagger for spines.
It is a modified form of the bidirection LSTM tagger included as an example in DyNet.

## Building

This requires several things to be installed already:

C++ Boost (I used 1.63.0)
DyNet
Eigen (used by DyNet)
Google Protocol Buffers

Note that while most of Boost is just header files, we use the serialization library, which must be compiled.

The makefile in this directory should generate the protocol buffer code and build the spine-tagger.

## Running

If you start with tokenised sentences there are several steps in the process:

### Adding sentence IDs

To take a file of tokenised lines and add sentence IDs in our format, run:

```Shell
python3 add_sent_id.py [starting number, default=1] < example-data.tok > example-data-with-ids.tok
```

This file is what the parser will be run with (so it knows the sentence IDs).
The reason we need IDs is that we can get the same sentence with different tag sequences (this occurs in the PTB annotations, so for oracle experiments using gold tags we need to be robust to it).

### Simplifying the text

We convert to lowercase, make all numbers 0, and add some placeholders needed for formatting reasons:

```Shell
python3 pre-process.py < example-data-with-ids.tok > example-data-with-ids.tok.simple
```

This runs on every file it is given as an argument and saves the new file to `<old-file>.clean`.

### Run the tagger

You will need the model and dictionaries available here (TODO).

```Shell
./spine-tagger -word-dict example.dict.tags -tag-dict example.dict.tags -model model.params -test example-data-with-ids.tok.simple -prefix example
```

Note, this does not actually return the 1-best tag sequence by default.
Instead, it generates tag distributions that can then be used for pruning.

## Options

- `-train corpus.txt` Set the training data
- `-dev dev.txt` Set the development data
- `-test test.txt` Set the test data
- `-model model.params` Read the model from this file (when doing training it is automatically created)
- `-word-dict dict.words` Read the word dictionary from this file (when doing training it is automatically created)
- `-tag-dict dict.tags` Read the tag dictionary from this file (when doing training it is automatically created)
- `-prefix name` Prefix to use for all output files on this run (default='tagger-experiment')
- `-cutoff-ratio CUTOFF_RATIO` When running in test mode, spines with scores below this ratio of the best will not be kept, which is useful for returning small files (default=0.5)
- `-layers LAYERS` Number of LSTM layers (default=2)
- `-input-dim INPUT_DIM` Dimensionality of the vectors used for words (default=128)
- `-hidden-dim HIDDEN_DIM` Dimensionality of the vectors in hidden states (default=256)
- `-tag-hidden-dim TAG_HIDDEN_DIM` Dimensionality of the vectors used for tags (default=64)
- `-print-labels` This will print the 1-best spines generated by the tagger

For training I suggest increasing the memory DyNet uses with `--dynet-mem 6000` and setting the number of threads for the Intel MKL library with `export MKL_NUM_THREADS=4`.

## TODO:

- Add step by step instructions on installing the necessary libraries
- Write a script to make the run process a one-step procedure
